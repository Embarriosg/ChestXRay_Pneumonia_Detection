{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a7b6964",
   "metadata": {},
   "source": [
    "# Transer Learning (ResNet 18)\n",
    "(Esto fijo se queda, mejor pregunta ever)\n",
    "Driven by the significance of depth, a question arises: Is\n",
    "learning better networks as easy as stacking more layers?[1] <br>\n",
    "Es una pregunta que se reliza en el paper Deep Residual Learning for Image Recognition donde se documento en 2015 la primera ResNet por parte del equipo de Microsoft, y eso es lo que hace una Red residual, al comienzo de el aprendizaje computacional las redes comvolucionales tenian un peor desempe√±o mediante se le agregaban mas capas, entonces la resNeet trata este problema saltando entre capas y dando como resultado?\n",
    "(borrador JAJAJAJ)\n",
    "- What is ResNet 18?\n",
    "As part of the ResNet family, ResNet-18 is the smallest and most lightweight model, making it a popular choice for fast experimentation, deployment, and educational use. Additionally, ResNet-18 is the goto model for image classification and is a reliable starting point balancing speed, accuracy, and simplicity. For longer training times and potentially better accuracy, within the family is ResNet-34, ResNet-50, ResNet-101, etc.<br>\n",
    "[Roboflow]https://blog-roboflow-com.translate.goog/resnet-18/?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es&_x_tr_pto=tc\n",
    "\n",
    "- Why we use it?\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65340e1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fb229f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core deep learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models # Crucial for Transfer Learning\n",
    "\n",
    "# Data handling and processing\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Visualization and Evaluation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# System and environment configuration\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Aesthetic settings for high-quality plots in GitHub\n",
    "plt.style.use('seaborn-v0_8-muted') \n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55950125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loader variable defined: True\n"
     ]
    }
   ],
   "source": [
    "# Go up one level to reach the project root and enter 'src'\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "# Import custom dataloaders\n",
    "from src.data.dataloaders import get_loaders\n",
    "\n",
    "# Execute the function to get data loaders\n",
    "# batch_size=32 is a stable standard for training\n",
    "train_loader, test_loader = get_loaders(batch_size=32, img_size=228)\n",
    "\n",
    "print(f\"train_loader variable defined: {train_loader is not None}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "322ccbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: Apple Silicon GPU (MPS)\n",
      "ResNet18 model was successfully adapted and migrated to device.\n"
     ]
    }
   ],
   "source": [
    "# 1. The pre-trained ResNet18 model was loaded with the latest available weights\n",
    "weights = models.ResNet18_Weights.DEFAULT\n",
    "model_tl = models.resnet18(weights=weights)\n",
    "\n",
    "# 2. Feature Extraction: All pre-trained parameters were frozen\n",
    "# This ensured that only the new custom layers were trained initially\n",
    "for param in model_tl.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 3. Input Layer Adaptation:\n",
    "# ResNet18 expects 3 channels (RGB), but chest X-rays are grayscale (1 channel).\n",
    "# The first convolutional layer was replaced to match our data dimensions.\n",
    "model_tl.conv1 = nn.Conv2d(\n",
    "    in_channels=1, \n",
    "    out_channels=64, \n",
    "    kernel_size=7, \n",
    "    stride=2, \n",
    "    padding=3, \n",
    "    bias=False\n",
    ")\n",
    "\n",
    "# 4. Classification Head Redesign:\n",
    "# The original 1000-class output was replaced with a binary classifier \n",
    "# (Normal vs Pneumonia) using a Dropout layer to mitigate overfitting.\n",
    "num_ftrs = model_tl.fc.in_features\n",
    "model_tl.fc = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(256, 2)\n",
    ")\n",
    "\n",
    "# Hardware selection (Optimized for Apple M1/M2/M3)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Training on: Apple Silicon GPU (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Training on: NVIDIA GPU (CUDA)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Training on: CPU\")\n",
    "\n",
    "# 5. Device Migration:\n",
    "# The model was moved to the Apple Silicon GPU (MPS) for hardware acceleration.\n",
    "model_tl = model_tl.to(device)\n",
    "\n",
    "print(\"ResNet18 model was successfully adapted and migrated to device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85088b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, criterion, optimizer, epochs=10):\n",
    "    \"\"\"\n",
    "    The training loop was implemented to optimize the model parameters \n",
    "    and evaluate performance on the test set.\n",
    "    \"\"\"\n",
    "    history = {'train_loss': [], 'test_loss': [], 'train_acc': [], 'test_acc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # --- TRAINING PHASE ---\n",
    "        model.train()\n",
    "        running_loss, correct_train, total_train = 0.0, 0, 0\n",
    "        \n",
    "        print(f\"\\n--- Epoch {epoch+1}/{epochs} ---\")\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            # Data was moved to the selected device (MPS/CUDA/CPU)\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Gradients were reset\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass: Predictions were generated\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass: Gradients were calculated and weights updated\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Training metrics were accumulated\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "            \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100 * correct_train / total_train\n",
    "        \n",
    "        # --- EVALUATION PHASE ---\n",
    "        model.eval()\n",
    "        test_loss, correct_test, total_test = 0.0, 0, 0\n",
    "        \n",
    "        # Gradient calculation was disabled for evaluation to save memory\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                test_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_test += labels.size(0)\n",
    "                correct_test += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_loss = test_loss / len(test_loader)\n",
    "        val_acc = 100 * correct_test / total_test\n",
    "        \n",
    "        # Epoch metrics were stored in history\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['test_loss'].append(val_loss)\n",
    "        history['train_acc'].append(epoch_acc)\n",
    "        history['test_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"Train Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.2f}%\")\n",
    "        print(f\"Test  Loss: {val_loss:.4f} | Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0a2b9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1/10 ---\n",
      "Train Loss: 0.4293 | Acc: 79.97%\n",
      "Test  Loss: 0.4551 | Acc: 77.88%\n",
      "\n",
      "--- Epoch 2/10 ---\n",
      "Train Loss: 0.2634 | Acc: 89.74%\n",
      "Test  Loss: 0.4943 | Acc: 78.69%\n",
      "\n",
      "--- Epoch 3/10 ---\n",
      "Train Loss: 0.2153 | Acc: 91.53%\n",
      "Test  Loss: 0.4082 | Acc: 83.01%\n",
      "\n",
      "--- Epoch 4/10 ---\n",
      "Train Loss: 0.1991 | Acc: 91.89%\n",
      "Test  Loss: 0.4896 | Acc: 80.13%\n",
      "\n",
      "--- Epoch 5/10 ---\n",
      "Train Loss: 0.1854 | Acc: 93.17%\n",
      "Test  Loss: 0.3956 | Acc: 84.13%\n",
      "\n",
      "--- Epoch 6/10 ---\n",
      "Train Loss: 0.1726 | Acc: 93.44%\n",
      "Test  Loss: 0.4152 | Acc: 83.81%\n",
      "\n",
      "--- Epoch 7/10 ---\n",
      "Train Loss: 0.1638 | Acc: 93.67%\n",
      "Test  Loss: 0.5624 | Acc: 79.33%\n",
      "\n",
      "--- Epoch 8/10 ---\n",
      "Train Loss: 0.1618 | Acc: 93.79%\n",
      "Test  Loss: 0.4880 | Acc: 82.53%\n",
      "\n",
      "--- Epoch 9/10 ---\n",
      "Train Loss: 0.1580 | Acc: 94.19%\n",
      "Test  Loss: 0.4658 | Acc: 82.85%\n",
      "\n",
      "--- Epoch 10/10 ---\n",
      "Train Loss: 0.1563 | Acc: 94.06%\n",
      "Test  Loss: 0.4851 | Acc: 82.85%\n"
     ]
    }
   ],
   "source": [
    "# The loss function remained CrossEntropyLoss for binary classification\n",
    "criterion_tl = nn.CrossEntropyLoss()\n",
    "\n",
    "# Only the modified layers were passed to the optimizer\n",
    "optimizer_tl = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model_tl.parameters()), \n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "# The training process was executed for 10 epochs\n",
    "NUM_EPOCHS = 10\n",
    "history_tl = train_model(\n",
    "    model_tl, \n",
    "    train_loader, \n",
    "    test_loader, \n",
    "    criterion_tl, \n",
    "    optimizer_tl, \n",
    "    epochs=NUM_EPOCHS\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
