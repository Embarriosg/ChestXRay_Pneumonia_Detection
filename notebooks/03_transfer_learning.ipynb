{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "620310d7",
   "metadata": {},
   "source": [
    "# Transer Learning (ResNet-18)\n",
    "In recent years, increasing the depth of neural networks has proven crucial for enhancing their learning capacity and performance in complex tasks, such as image recognition. However, training increasingly deep networks poses significant technical challenges, particularly related to the propagation of information and gradients during training.\n",
    "\n",
    "Kaiming He and collaborators stated: \"Driven by the significance of depth, a question arises: Is\n",
    "learning better networks as easy as stacking more layers?\" \n",
    "\n",
    "This question is precisely what ResNet addresses, a convolutional neural network (CNN) architecture whose number of layers and parameters is indicated in its name. Introduced in 2015 by Microsoft, ResNet offered a solution to the vanishing gradient problem that occurred when adding extra layers, through the use of **skip connections**. The ResNet family includes models of various depths, such as ResNet-18, ResNet-34, ResNet-50, ResNet-101, and ResNet-152, each adapted to different levels of complexity and learning capacity.\n",
    "\n",
    "You can find more details in the original [ResNet paper](https://arxiv.org/abs/1512.03385)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d68fe9",
   "metadata": {},
   "source": [
    "### 1. Library Imports for Transfer Learning\n",
    "\n",
    "For the implementation of the second model, the core PyTorch libraries were utilized. Specifically, the models sub-module from torchvision was imported to access the ResNet18 architecture and its pre-trained weights. Additionally, seaborn was included to enhance the visualization of the final comparison metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fb229f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core deep learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models # Crucial for Transfer Learning\n",
    "\n",
    "# Data handling and processing\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Visualization and Evaluation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# System and environment configuration\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Aesthetic settings for high-quality plots in GitHub\n",
    "plt.style.use('seaborn-v0_8-muted') \n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d6c6cd",
   "metadata": {},
   "source": [
    "### 2. Data Loading and Preprocessing\n",
    "\n",
    "The system paths were configured to access local modules and the dataloaders were initialized. An image size of 228x228 was selected to balance detail and computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55950125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loader variable defined: True\n"
     ]
    }
   ],
   "source": [
    "# Go up one level to reach the project root and enter 'src'\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "# Import custom dataloaders\n",
    "from src.data.dataloaders import get_loaders\n",
    "\n",
    "# Execute the function to get data loaders\n",
    "# batch_size=32 is a stable standard for training\n",
    "train_loader, test_loader = get_loaders(batch_size=32, img_size=228)\n",
    "\n",
    "print(f\"train_loader variable defined: {train_loader is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3d1e43",
   "metadata": {},
   "source": [
    "### 3. ResNet18 Implementation and Customization\n",
    "\n",
    "The ResNet18 architecture was adapted to serve as the primary model. Since the original model was trained on the ImageNet dataset (RGB images), the input layer was modified to process single-channel grayscale X-rays. Furthermore, the weights of the convolutional base were frozen to prevent their distortion during the initial training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "322ccbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: Apple Silicon GPU (MPS)\n",
      "ResNet18 model was successfully adapted and migrated to device.\n"
     ]
    }
   ],
   "source": [
    "# 1. The pre-trained ResNet18 model was loaded with the latest available weights\n",
    "weights = models.ResNet18_Weights.DEFAULT\n",
    "model_tl = models.resnet18(weights=weights)\n",
    "\n",
    "# 2. Feature Extraction: All pre-trained parameters were frozen\n",
    "# This ensured that only the new custom layers were trained initially\n",
    "for param in model_tl.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 3. Input Layer Adaptation:\n",
    "# ResNet18 expects 3 channels (RGB), but chest X-rays are grayscale (1 channel).\n",
    "# The first convolutional layer was replaced to match our data dimensions.\n",
    "model_tl.conv1 = nn.Conv2d(\n",
    "    in_channels=1, \n",
    "    out_channels=64, \n",
    "    kernel_size=7, \n",
    "    stride=2, \n",
    "    padding=3, \n",
    "    bias=False\n",
    ")\n",
    "\n",
    "# 4. Classification Head Redesign:\n",
    "# The original 1000-class output was replaced with a binary classifier \n",
    "# (Normal vs Pneumonia) using a Dropout layer to mitigate overfitting.\n",
    "num_ftrs = model_tl.fc.in_features\n",
    "model_tl.fc = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(256, 2)\n",
    ")\n",
    "\n",
    "# Hardware selection (Optimized for Apple M1/M2/M3)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Training on: Apple Silicon GPU (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Training on: NVIDIA GPU (CUDA)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Training on: CPU\")\n",
    "\n",
    "# 5. Device Migration:\n",
    "# The model was moved to the Apple Silicon GPU (MPS) for hardware acceleration.\n",
    "model_tl = model_tl.to(device)\n",
    "\n",
    "print(\"ResNet18 model was successfully adapted and migrated to device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c2a548",
   "metadata": {},
   "source": [
    "### 4. Training Loop Definition\n",
    "\n",
    "A function was created to manage the training process, error calculation, and weight optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85088b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, criterion, optimizer, epochs=10):\n",
    "    \"\"\"\n",
    "    The training loop was implemented to optimize the model parameters \n",
    "    and evaluate performance on the test set.\n",
    "    \"\"\"\n",
    "    history = {'train_loss': [], 'test_loss': [], 'train_acc': [], 'test_acc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # --- TRAINING PHASE ---\n",
    "        model.train()\n",
    "        running_loss, correct_train, total_train = 0.0, 0, 0\n",
    "        \n",
    "        print(f\"\\n--- Epoch {epoch+1}/{epochs} ---\")\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            # Data was moved to the selected device (MPS/CUDA/CPU)\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Gradients were reset\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass: Predictions were generated\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass: Gradients were calculated and weights updated\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Training metrics were accumulated\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "            \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100 * correct_train / total_train\n",
    "        \n",
    "        # --- EVALUATION PHASE ---\n",
    "        model.eval()\n",
    "        test_loss, correct_test, total_test = 0.0, 0, 0\n",
    "        \n",
    "        # Gradient calculation was disabled for evaluation to save memory\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                test_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_test += labels.size(0)\n",
    "                correct_test += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_loss = test_loss / len(test_loader)\n",
    "        val_acc = 100 * correct_test / total_test\n",
    "        \n",
    "        # Epoch metrics were stored in history\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['test_loss'].append(val_loss)\n",
    "        history['train_acc'].append(epoch_acc)\n",
    "        history['test_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"Train Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.2f}%\")\n",
    "        print(f\"Test  Loss: {val_loss:.4f} | Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7069b733",
   "metadata": {},
   "source": [
    "### 5. Hyperparameter Setup and Training\n",
    "\n",
    "The optimization strategy focused on the newly added layers. The Adam optimizer was configured to update only the parameters where requires_grad was set to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0a2b9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1/10 ---\n",
      "Train Loss: 0.4293 | Acc: 79.97%\n",
      "Test  Loss: 0.4551 | Acc: 77.88%\n",
      "\n",
      "--- Epoch 2/10 ---\n",
      "Train Loss: 0.2634 | Acc: 89.74%\n",
      "Test  Loss: 0.4943 | Acc: 78.69%\n",
      "\n",
      "--- Epoch 3/10 ---\n",
      "Train Loss: 0.2153 | Acc: 91.53%\n",
      "Test  Loss: 0.4082 | Acc: 83.01%\n",
      "\n",
      "--- Epoch 4/10 ---\n",
      "Train Loss: 0.1991 | Acc: 91.89%\n",
      "Test  Loss: 0.4896 | Acc: 80.13%\n",
      "\n",
      "--- Epoch 5/10 ---\n",
      "Train Loss: 0.1854 | Acc: 93.17%\n",
      "Test  Loss: 0.3956 | Acc: 84.13%\n",
      "\n",
      "--- Epoch 6/10 ---\n",
      "Train Loss: 0.1726 | Acc: 93.44%\n",
      "Test  Loss: 0.4152 | Acc: 83.81%\n",
      "\n",
      "--- Epoch 7/10 ---\n",
      "Train Loss: 0.1638 | Acc: 93.67%\n",
      "Test  Loss: 0.5624 | Acc: 79.33%\n",
      "\n",
      "--- Epoch 8/10 ---\n",
      "Train Loss: 0.1618 | Acc: 93.79%\n",
      "Test  Loss: 0.4880 | Acc: 82.53%\n",
      "\n",
      "--- Epoch 9/10 ---\n",
      "Train Loss: 0.1580 | Acc: 94.19%\n",
      "Test  Loss: 0.4658 | Acc: 82.85%\n",
      "\n",
      "--- Epoch 10/10 ---\n",
      "Train Loss: 0.1563 | Acc: 94.06%\n",
      "Test  Loss: 0.4851 | Acc: 82.85%\n"
     ]
    }
   ],
   "source": [
    "# The loss function remained CrossEntropyLoss for binary classification\n",
    "criterion_tl = nn.CrossEntropyLoss()\n",
    "\n",
    "# Only the modified layers were passed to the optimizer\n",
    "optimizer_tl = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model_tl.parameters()), \n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "# The training process was executed for 10 epochs\n",
    "NUM_EPOCHS = 10\n",
    "history_tl = train_model(\n",
    "    model_tl, \n",
    "    train_loader, \n",
    "    test_loader, \n",
    "    criterion_tl, \n",
    "    optimizer_tl, \n",
    "    epochs=NUM_EPOCHS\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
